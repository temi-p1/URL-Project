{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests_html'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\JOSHAL~1\\AppData\\Local\\Temp/ipykernel_31952/1930261044.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0murlparse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrequests_html\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTMLSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# sample youtube video url\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mvideo_url\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://www.youtube.com/watch?v=dQw4w9WgXcQ'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'requests_html'"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from requests_html import HTMLSession\n",
    "# sample youtube video url\n",
    "video_url = 'https://www.youtube.com/watch?v=dQw4w9WgXcQ'\n",
    "# init an HTML Session\n",
    "session = HTMLSession()\n",
    "# get the html content\n",
    "response = session.get(video_url)\n",
    "# execute Java-script\n",
    "response.html.render(sleep=1)\n",
    "# create bs object to parse HTML\n",
    "soup = bs(response.html.html, \"html.parser\")\n",
    "\n",
    "urls = ['https://www.youtube.com/watch?v=dQw4w9WgXcQ']\n",
    "response = requests.get(urls[0]) # send GET request to url\n",
    "#soup = BeautifulSoup(response.text, 'html.parser') # parse the text given by request\n",
    "vidTitle = soup.title.text\n",
    "# print(soup.prettify())\n",
    "soup.find(\"meta\", itemprop=\"name\")[\"content\"] # title\n",
    "soup.find(\"meta\", itemprop=\"interactionCount\")['content'] # view count\n",
    "soup.find(\"meta\", itemprop=\"description\")['content'] # description\n",
    "soup.find(\"meta\", itemprop=\"datePublished\")['content'] # video publish date\n",
    "soup.find(\"span\", {\"class\": \"ytp-time-duration\"}) # video length\n",
    "', '.join([ meta.attrs.get(\"content\") for meta in soup.find_all(\"meta\", {\"property\": \"og:video:tag\"}) ]) # video tags\n",
    "\n",
    "# channel details\n",
    "channel_tag = soup.find(\"yt-formatted-string\", {\"class\": \"ytd-channel-name\"}).find(\"a\")\n",
    "# channel name\n",
    "channel_name = channel_tag.text\n",
    "# channel URL\n",
    "channel_url = f\"https://www.youtube.com{channel_tag['href']}\"\n",
    "# number of subscribers as str\n",
    "channel_subscribers = soup.find(\"yt-formatted-string\", {\"id\": \"owner-sub-count\"}).text.strip()\n",
    "result = {'name': channel_name, 'url': channel_url, 'subscribers': channel_subscribers}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covcom fctn\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "\n",
    "urls = ['https://www.youtube.com/watch?v=dQw4w9WgXcQ']\n",
    "\n",
    "#provider_details = pd.DataFrame(columns=['company_name','company_link', 'company_number', 'company_email'])\n",
    "\n",
    "for url in urls:    \n",
    "    # scrape page\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    providers_table = soup.find('table', {'class', 'govuk-table'}).find('tbody')\n",
    "\n",
    "    url_provider_details = pd.DataFrame(columns=['company_name', 'company_link', 'company_number', 'company_email'])\n",
    "    for row in providers_table.find_all('tr'):\n",
    "        # test provider saved in cell with id 'provider'\n",
    "        provider = row.find(id='provider').find('a')\n",
    "        name = provider.get_text().rstrip().lower()\n",
    "        link = provider['href']\n",
    "    \n",
    "        # test provider number and email saved in only cell(s) with no id\n",
    "        number_email = row.find_all('td', id=None)\n",
    "        \n",
    "        # remove web archive prefix from wayback machine\n",
    "        link = link[43:] if 'web.archive.org' in link else link\n",
    "\n",
    "        # old format stores number and email in separate cells\n",
    "        # new format stores number and email in same cell\n",
    "        if len(number_email) == 1:\n",
    "            number_email = number_email[0].find_all('a')\n",
    "        number = str(number_email[0].get_text())\n",
    "        email = number_email[1].get_text()\n",
    "        \n",
    "        # apply standard format to numbers, emails and links\n",
    "        if number and len(number) >= 10:\n",
    "            number = ' '.join(number.rstrip().split())\n",
    "            number = phonenumbers.format_number(phonenumbers.parse(number, 'GB'), phonenumbers.PhoneNumberFormat.INTERNATIONAL)\n",
    "        else:\n",
    "            number = np.nan \n",
    "\n",
    "        if email:\n",
    "            email = str(email).lower()\n",
    "\n",
    "        if link:\n",
    "            link = urlparse(link.lower()).netloc\n",
    "\n",
    "        url_provider_details = url_provider_details.append({\n",
    "            'company_name': name,\n",
    "            'company_link': link,\n",
    "            'company_number': number,\n",
    "            'company_email': email\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    provider_details = pd.merge(\n",
    "        provider_details, \n",
    "        url_provider_details, \n",
    "        how=\"outer\", \n",
    "        on=['company_name','company_link','company_number','company_email']\n",
    "    )\n",
    "\n",
    "provider_details.to_csv('datasets/provider_details.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e278ca4e9a2ae999502812da3aa9042213e3ce53c46b5f0452fc22ced78fb5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
