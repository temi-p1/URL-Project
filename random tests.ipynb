{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup get visible text from webpages (unreliable)\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.request\n",
    "\n",
    "# indicator of whether element is visible based on tag\n",
    "# this is just a guess and is not consistent across all sites\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "# func to get text given html\n",
    "def text_from_html(body):\n",
    "    soup = BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "urls = ['http://www.nytimes.com/2009/12/21/us/21storm.html','https://www.youtube.com/']\n",
    "\n",
    "# overall func to get text given list of urls\n",
    "def getText1(urls):\n",
    "    texts = []\n",
    "    for url in urls:\n",
    "        html = urllib.request.urlopen(url).read()\n",
    "        texts.append(text_from_html(html))\n",
    "    return texts\n",
    "\n",
    "print(getText1(urls)[0]) # works well for news site\n",
    "print(getText1(urls)[1]) # works terribly for youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BeautifulSoup get visible text from webpages (written more efficiently) (unreliable)\n",
    "\n",
    "urls = ['http://www.nytimes.com/2009/12/21/us/21storm.html','https://www.youtube.com/']\n",
    "\n",
    "def getText2(urls):\n",
    "    texts = []\n",
    "    for url in urls:\n",
    "        html = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(html)\n",
    "        [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "        texts.append(soup.getText())\n",
    "    return texts\n",
    "\n",
    "print(getText2(urls))\n",
    "# extracted text isnt as nice as getText1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenium get text\n",
    "from selenium import webdriver\n",
    "exec_path = r\"C:\\Users\\JoshAlder\\OneDrive - Principle One\\Documents\\VS Code\\URLs Project New\\Rando\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(executable_path=exec_path)\n",
    "driver.get(\"https://www.bbc.co.uk/news/business-58830955\")\n",
    "driver.get('https://www.youtube.com/')\n",
    "el = driver.find_element_by_tag_name('body').text\n",
    "print(el)\n",
    "# driver.close()\n",
    "# This script cant get past sign in pop up whereas BS seems to be able to (NYTimes Article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapy (faster than other libraries)\n",
    "# selectors - still need to specify tags & locations in HTML?\n",
    "import scrapy\n",
    "\n",
    "class TestSpider(scrapy.Spider):\n",
    "    name = 'test'\n",
    "    start_urls = [\"https://www.bbc.co.uk/news/business-58830955\"]\n",
    "\n",
    "    def parse(self,response):\n",
    "        links = response.xpath('//body//text()')\n",
    "        html = \"\"\n",
    "        for link in links:\n",
    "            url = links.get()\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inscriptis get text - works well for lots of sites but struggles with youtube for some reason\n",
    "import urllib.request \n",
    "from inscriptis import get_text \n",
    " \n",
    "urls = [\"https://www.amazon.co.uk/introducing-fire-tv-stick-lite-with-alexa-voice-remote-lite-no-tv-controls-2020-release/dp/B07ZZW7QCM/?_encoding=UTF8&pd_rd_w=momYP&pf_rd_p=7b33cd3c-1a87-4db8-9f31-ba1adc449805&pf_rd_r=V3TTH21TYKK6REAYQ6TR&pd_rd_r=bc8c40c9-546b-4c63-8b4a-29fd04e2f958&pd_rd_wg=06pzc&ref_=pd_gw_unk\",\"https://www.bbc.co.uk/news/business-58830955\"]\n",
    "html = urllib.request.urlopen(urls[0]).read().decode('utf-8')\n",
    "text = get_text(html)\n",
    "\n",
    "print(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloudscraper (can apparently get around protection? particularly cloudflare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google API website categorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Meta - Selenium/BS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Youtube Scraping Tests\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "urls = ['https://www.youtube.com/watch?v=dQw4w9WgXcQ']\n",
    "response = requests.get(urls[0]) # send GET request to url\n",
    "soup = BeautifulSoup(response.text, 'html.parser') # parse the text given by request\n",
    "vidTitle = soup.title.text\n",
    "print(soup.prettify())\n",
    "# print(soup.find(class='style-scope ytd-video-primary-info-renderer')) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Youtube Detailed Scraping \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "\n",
    "urls = ['https://www.youtube.com/watch?v=dQw4w9WgXcQ']\n",
    "\n",
    "#provider_details = pd.DataFrame(columns=['company_name','company_link', 'company_number', 'company_email'])\n",
    "\n",
    "for url in urls:    \n",
    "    # scrape page\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    providers_table = soup.find('table', {'class', 'govuk-table'}).find('tbody')\n",
    "\n",
    "    url_provider_details = pd.DataFrame(columns=['company_name', 'company_link', 'company_number', 'company_email'])\n",
    "    for row in providers_table.find_all('tr'):\n",
    "        # test provider saved in cell with id 'provider'\n",
    "        provider = row.find(id='provider').find('a')\n",
    "        name = provider.get_text().rstrip().lower()\n",
    "        link = provider['href']\n",
    "    \n",
    "        # test provider number and email saved in only cell(s) with no id\n",
    "        number_email = row.find_all('td', id=None)\n",
    "        \n",
    "        # remove web archive prefix from wayback machine\n",
    "        link = link[43:] if 'web.archive.org' in link else link\n",
    "\n",
    "        # old format stores number and email in separate cells\n",
    "        # new format stores number and email in same cell\n",
    "        if len(number_email) == 1:\n",
    "            number_email = number_email[0].find_all('a')\n",
    "        number = str(number_email[0].get_text())\n",
    "        email = number_email[1].get_text()\n",
    "        \n",
    "        # apply standard format to numbers, emails and links\n",
    "        if number and len(number) >= 10:\n",
    "            number = ' '.join(number.rstrip().split())\n",
    "            number = phonenumbers.format_number(phonenumbers.parse(number, 'GB'), phonenumbers.PhoneNumberFormat.INTERNATIONAL)\n",
    "        else:\n",
    "            number = np.nan \n",
    "\n",
    "        if email:\n",
    "            email = str(email).lower()\n",
    "\n",
    "        if link:\n",
    "            link = urlparse(link.lower()).netloc\n",
    "\n",
    "        url_provider_details = url_provider_details.append({\n",
    "            'company_name': name,\n",
    "            'company_link': link,\n",
    "            'company_number': number,\n",
    "            'company_email': email\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    provider_details = pd.merge(\n",
    "        provider_details, \n",
    "        url_provider_details, \n",
    "        how=\"outer\", \n",
    "        on=['company_name','company_link','company_number','company_email']\n",
    "    )\n",
    "\n",
    "provider_details.to_csv('datasets/provider_details.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube pop-up closing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traceback / indexing\n",
    "# add screenshot metadata?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tor URL processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url matching\n",
    "# maybe match from a general list, which can be the url list itself (duplicates) or white/red lists?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL prioritisation and filtering\n",
    "# time and no. visits will be given in the history excel document obtained by police from devices"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e278ca4e9a2ae999502812da3aa9042213e3ce53c46b5f0452fc22ced78fb5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
