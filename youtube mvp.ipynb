{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Rick Astley - Never Gonna Give You Up (Officia...   \n",
      "\n",
      "                                          Link  Genre  View Count  \\\n",
      "0  https://www.youtube.com/watch?v=dQw4w9WgXcQ  Music  1067425485   \n",
      "\n",
      "                                   Channel Link  \\\n",
      "0  youtube.com/channel/UCuAXFkgsw1L7xaCfnd5JJOw   \n",
      "\n",
      "                                         Description Upload Date  \\\n",
      "0  The official video for â€œNever Gonna Give You U...  2009-10-24   \n",
      "\n",
      "  Family Friendly                                               Tags  \n",
      "0            true  rick astley, Never Gonna Give You Up, nggyu, n...  \n"
     ]
    }
   ],
   "source": [
    "# BeautifulSoup\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "urls = ['https://www.youtube.com/watch?v=dQw4w9WgXcQ']\n",
    "response = requests.get(urls[0]) # send GET request to url\n",
    "soup = BeautifulSoup(response.text, 'html.parser') # parse the text via html parser\n",
    "# soup = BeautifulSoup(response.text,'lxml') # parse text via lxml\n",
    "video = {}\n",
    "\n",
    "# video['Title'] = soup.title.text\n",
    "# print(soup.prettify())\n",
    "# print(soup.find_all(\"meta\")) # show all meta\n",
    "video['Title'] = soup.find(\"meta\", itemprop=\"name\")[\"content\"] # title\n",
    "video['Link'] = urls[0]\n",
    "video['Genre'] = soup.find(\"meta\", itemprop=\"genre\")['content'] # view count\n",
    "video['View Count'] = soup.find(\"meta\", itemprop=\"interactionCount\")['content'] # view count\n",
    "video['Channel Link'] = 'youtube.com/channel/'+soup.find(\"meta\", itemprop=\"channelId\")['content'] # channel link\n",
    "video['Description'] = soup.find(\"meta\", itemprop=\"description\")['content'] # description\n",
    "video['Upload Date'] = soup.find(\"meta\", itemprop=\"uploadDate\")['content'] # video publish date\n",
    "video['Family Friendly'] = soup.find(\"meta\", itemprop=\"isFamilyFriendly\")['content'] # video publish date\n",
    "# video['Duration'] = soup.find(\"span\", {\"class\": \"ytp-time-duration\"}) # doesn't work\n",
    "video['Tags'] = ', '.join([ meta.attrs.get(\"content\") for meta in soup.find_all(\"meta\", {\"property\": \"og:video:tag\"}) ]) # video tags\n",
    "\n",
    "# print(soup.find_all(\"yt-formatted-string\", {\"class\",\"content style-scope ytd-secondary-info-renderer\"}))\n",
    "## can't locate description as shown in inspect element perhaps due to page using JS to render content (BS can't run JS)\n",
    "\n",
    "\n",
    "df = pd.DataFrame(video, columns = [*video], index = [0])\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Can get basic data but cannot get full description... \n",
    "# YouTube generates lots of content using JS - i think this is why BS can't find stuff I found via inspect element\n",
    "# managed to find data through meta but not everything, may need Selenium to handle JS rendering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube Data API - quota of 10,000 units per day (one read operation 'usually costs 1 unit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# covcom fctn for copying\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import numpy as np\n",
    "\n",
    "urls = ['https://www.youtube.com/watch?v=dQw4w9WgXcQ']\n",
    "\n",
    "#provider_details = pd.DataFrame(columns=['company_name','company_link', 'company_number', 'company_email'])\n",
    "\n",
    "for url in urls:    \n",
    "    # scrape page\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    providers_table = soup.find('table', {'class', 'govuk-table'}).find('tbody')\n",
    "\n",
    "    url_provider_details = pd.DataFrame(columns=['company_name', 'company_link', 'company_number', 'company_email'])\n",
    "    for row in providers_table.find_all('tr'):\n",
    "        # test provider saved in cell with id 'provider'\n",
    "        provider = row.find(id='provider').find('a')\n",
    "        name = provider.get_text().rstrip().lower()\n",
    "        link = provider['href']\n",
    "    \n",
    "        # test provider number and email saved in only cell(s) with no id\n",
    "        number_email = row.find_all('td', id=None)\n",
    "        \n",
    "        # remove web archive prefix from wayback machine\n",
    "        link = link[43:] if 'web.archive.org' in link else link\n",
    "\n",
    "        # old format stores number and email in separate cells\n",
    "        # new format stores number and email in same cell\n",
    "        if len(number_email) == 1:\n",
    "            number_email = number_email[0].find_all('a')\n",
    "        number = str(number_email[0].get_text())\n",
    "        email = number_email[1].get_text()\n",
    "        \n",
    "        # apply standard format to numbers, emails and links\n",
    "        if number and len(number) >= 10:\n",
    "            number = ' '.join(number.rstrip().split())\n",
    "            number = phonenumbers.format_number(phonenumbers.parse(number, 'GB'), phonenumbers.PhoneNumberFormat.INTERNATIONAL)\n",
    "        else:\n",
    "            number = np.nan \n",
    "\n",
    "        if email:\n",
    "            email = str(email).lower()\n",
    "\n",
    "        if link:\n",
    "            link = urlparse(link.lower()).netloc\n",
    "\n",
    "        url_provider_details = url_provider_details.append({\n",
    "            'company_name': name,\n",
    "            'company_link': link,\n",
    "            'company_number': number,\n",
    "            'company_email': email\n",
    "        }, ignore_index=True)\n",
    "\n",
    "    provider_details = pd.merge(\n",
    "        provider_details, \n",
    "        url_provider_details, \n",
    "        how=\"outer\", \n",
    "        on=['company_name','company_link','company_number','company_email']\n",
    "    )\n",
    "\n",
    "provider_details.to_csv('datasets/provider_details.csv')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1e278ca4e9a2ae999502812da3aa9042213e3ce53c46b5f0452fc22ced78fb5d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
